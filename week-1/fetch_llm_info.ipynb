{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File used to fetch various info about common tokenizers found in huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.17 (from transformers)\n",
      "  Downloading numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/saavajuu/anaconda3/envs/llm_env/lib/python3.11/site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Collecting requests (from transformers)\n",
      "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.6-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.23.2->transformers)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/saavajuu/anaconda3/envs/llm_env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading transformers-4.46.1-py3-none-any.whl (10.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.26.2-py3-none-any.whl (447 kB)\n",
      "Downloading numpy-2.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (762 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m763.0/763.0 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m792.8/792.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n",
      "Downloading tokenizers-0.20.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.6-py3-none-any.whl (78 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Downloading charset_normalizer-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, numpy, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ansible-core 2.17.5 requires cryptography, which is not installed.\n",
      "ansible-core 2.17.5 requires jinja2>=3.0.0, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed certifi-2024.8.30 charset-normalizer-3.4.0 filelock-3.16.1 fsspec-2024.10.0 huggingface-hub-0.26.2 idna-3.10 numpy-2.1.2 pyyaml-6.0.2 regex-2024.9.11 requests-2.32.3 safetensors-0.4.5 tokenizers-0.20.1 tqdm-4.66.6 transformers-4.46.1 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "# setup your conda // venv , to be able to use necessary libraries\n",
    "# make sure to login via huggingface-cli for accesing all repos\n",
    "\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model meta-llama/Meta-Llama-3-8B-Instruct: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct.\n",
      "403 Client Error. (Request ID: Root=1-6724adaa-78356bf0768c79012831acc4;6da7a15d-fa04-4945-9540-cd76013d6705)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/resolve/main/config.json.\n",
      "Access to model meta-llama/Meta-Llama-3-8B-Instruct is restricted and you are not in the authorized list. Visit https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct to ask for access.\n",
      "                                 Model           Tokenizer Type  \\\n",
      "0           Qwen/Qwen2.5-1.5B-Instruct       Qwen2TokenizerFast   \n",
      "1            meta-llama/Llama-3.1-405B  PreTrainedTokenizerFast   \n",
      "2                openai-community/gpt2        GPT2TokenizerFast   \n",
      "3                    google/gemma-2-2B       GemmaTokenizerFast   \n",
      "4               bigscience/bloomz-560m       BloomTokenizerFast   \n",
      "5                    facebook/opt-125m        GPT2TokenizerFast   \n",
      "6                distilbert/distilgpt2        GPT2TokenizerFast   \n",
      "7  meta-llama/Meta-Llama-3-8B-Instruct                      N/A   \n",
      "8   mistralai/Mistral-7B-Instruct-v0.2       LlamaTokenizerFast   \n",
      "9     microsoft/Phi-3-mini-4k-instruct       LlamaTokenizerFast   \n",
      "\n",
      "  Vocabulary Size                                     Special Tokens  \n",
      "0          151643  {'eos_token': '<|im_end|>', 'pad_token': '<|en...  \n",
      "1          128000  {'bos_token': '<|begin_of_text|>', 'eos_token'...  \n",
      "2           50257  {'bos_token': '<|endoftext|>', 'eos_token': '<...  \n",
      "3          256000  {'bos_token': '<bos>', 'eos_token': '<eos>', '...  \n",
      "4          250680  {'bos_token': '<s>', 'eos_token': '</s>', 'unk...  \n",
      "5           50265  {'bos_token': '</s>', 'eos_token': '</s>', 'un...  \n",
      "6           50257  {'bos_token': '<|endoftext|>', 'eos_token': '<...  \n",
      "7             N/A                                                N/A  \n",
      "8           32000  {'bos_token': '<s>', 'eos_token': '</s>', 'unk...  \n",
      "9           32000  {'bos_token': '<s>', 'eos_token': '<|endoftext...  \n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import pandas as pd\n",
    "\n",
    "# Some of the most downloaded text generating models from the Hugging Face Model Hub \n",
    "# https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
    "model_names = [\n",
    "    \"Qwen/Qwen2.5-1.5B-Instruct\",\n",
    "    \"meta-llama/Llama-3.1-405B\",\n",
    "    \"openai-community/gpt2\",\n",
    "    \"google/gemma-2-2B\",\n",
    "    \"bigscience/bloomz-560m\",\n",
    "    \"facebook/opt-125m\",\n",
    "    \"distilbert/distilgpt2\",\n",
    "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"microsoft/Phi-3-mini-4k-instruct\"    \n",
    "]\n",
    "\n",
    "tokenizer_data = {\n",
    "    \"Model\": [],\n",
    "    \"Tokenizer Type\": [],\n",
    "    \"Vocabulary Size\": [],\n",
    "    \"Special Tokens\": [],\n",
    "}\n",
    "\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        \n",
    "        tokenizer_type = type(tokenizer).__name__\n",
    "        vocab_size = tokenizer.vocab_size\n",
    "        special_tokens = tokenizer.special_tokens_map\n",
    "        \n",
    "        tokenizer_data[\"Model\"].append(model_name)\n",
    "        tokenizer_data[\"Tokenizer Type\"].append(tokenizer_type)\n",
    "        tokenizer_data[\"Vocabulary Size\"].append(vocab_size)\n",
    "        tokenizer_data[\"Special Tokens\"].append(special_tokens)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model {model_name}: {e}\")\n",
    "        tokenizer_data[\"Model\"].append(model_name)\n",
    "        tokenizer_data[\"Tokenizer Type\"].append(\"N/A\")\n",
    "        tokenizer_data[\"Vocabulary Size\"].append(\"N/A\")\n",
    "        tokenizer_data[\"Special Tokens\"].append(\"N/A\")\n",
    "\n",
    "df = pd.DataFrame(tokenizer_data)\n",
    "print(df)\n",
    "df.to_csv(\"huggingface_model_tokenizers.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
